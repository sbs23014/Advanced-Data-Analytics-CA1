{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85631b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load the training data from HDFS\n",
    "\n",
    "import pickle\n",
    "import hdfs\n",
    "\n",
    "# Load the CIFAR10 dataset from HDFS\n",
    "client = hdfs.InsecureClient('http://localhost:9870')\n",
    "with client.read('/cifar10/cifar10/data_batch_1') as f:\n",
    "    data_batch_1_dict = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "client = hdfs.InsecureClient('http://localhost:9870')\n",
    "with client.read('/cifar10/cifar10/data_batch_2') as f:\n",
    "    data_batch_2_dict = pickle.load(f, encoding='latin1')\n",
    "\n",
    "client = hdfs.InsecureClient('http://localhost:9870')\n",
    "with client.read('/cifar10/cifar10/data_batch_3') as f:\n",
    "    data_batch_3_dict = pickle.load(f, encoding='latin1')\n",
    "\n",
    "client = hdfs.InsecureClient('http://localhost:9870')\n",
    "with client.read('/cifar10/cifar10/data_batch_4') as f:\n",
    "    data_batch_4_dict = pickle.load(f, encoding='latin1')\n",
    "\n",
    "client = hdfs.InsecureClient('http://localhost:9870')\n",
    "with client.read('/cifar10/cifar10/data_batch_5') as f:\n",
    "    data_batch_5_dict = pickle.load(f, encoding='latin1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a1dab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(data_batch_1_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2055d4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_label <class 'str'>\n",
      "labels <class 'list'>\n",
      "data <class 'numpy.ndarray'>\n",
      "filenames <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for item in data_batch_1_dict:\n",
    "    print(item, type(data_batch_1_dict[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1eddbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels:\", set(data_batch_1_dict['labels'])) #This is each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2460065d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['num_cases_per_batch', 'label_names', 'num_vis'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = hdfs.InsecureClient('http://localhost:9870')\n",
    "with client.read('/cifar10/batches.meta') as f:\n",
    "    Meta = pickle.load(f, encoding='latin1')\n",
    "\n",
    "print(type(Meta))\n",
    "print(Meta.keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04c7218c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Names: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "print(\"Label Names:\", Meta['label_names'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7bbb030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072,)\n"
     ]
    }
   ],
   "source": [
    "image = data_batch_1_dict['data'][0]\n",
    "print(image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "984f3f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "image = image.reshape(3,32,32)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7f481f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "image = image.transpose(1,2,0)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d100aa0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'frog')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# label names\n",
    "label_name = Meta['label_names']\n",
    "# take first image\n",
    "image = data_batch_1_dict['data'][0]\n",
    "# take first image label index\n",
    "label = data_batch_1_dict['labels'][0]\n",
    "# Reshape the image\n",
    "image = image.reshape(3,32,32)\n",
    "# Transpose the image\n",
    "image = image.transpose(1,2,0)\n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "plt.title(label_name[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "242d24d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_batch_1_array= data_batch_1_dict['data']\n",
    "data_batch_2_array= data_batch_2_dict['data']\n",
    "data_batch_3_array= data_batch_3_dict['data']\n",
    "data_batch_4_array= data_batch_4_dict['data']\n",
    "data_batch_5_array= data_batch_5_dict['data']\n",
    "Merged_trained_data = np.concatenate((data_batch_1_array, data_batch_2_array, data_batch_3_array, data_batch_4_array, data_batch_5_array), axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eac7ea76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3072)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Merged_trained_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5242fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bring in the test data and create XTrain, yTrain, XTest, yTest\n",
    "\n",
    "client = hdfs.InsecureClient('http://localhost:9870')\n",
    "with client.read('/cifar10/cifar10/test_batch') as f:\n",
    "    test_batch = pickle.load(f, encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4dbde66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 59,  43,  50, ..., 140,  84,  72],\n",
       "       [154, 126, 105, ..., 139, 142, 144],\n",
       "       [255, 253, 253, ...,  83,  83,  84],\n",
       "       ...,\n",
       "       [ 35,  40,  42, ...,  77,  66,  50],\n",
       "       [189, 186, 185, ..., 169, 171, 171],\n",
       "       [229, 236, 234, ..., 173, 162, 161]], dtype=uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Train = Merged_trained_data\n",
    "X_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e365a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77589c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 9, 9, ..., 9, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_batch_1_array= data_batch_1_dict['labels']\n",
    "labels_batch_2_array= data_batch_2_dict['labels']\n",
    "labels_batch_3_array= data_batch_3_dict['labels']\n",
    "labels_batch_4_array= data_batch_4_dict['labels']\n",
    "labels_batch_5_array= data_batch_5_dict['labels']\n",
    "Merged_trained_labels = np.concatenate((labels_batch_1_array, labels_batch_2_array, labels_batch_3_array, labels_batch_4_array, labels_batch_5_array), axis=0)\n",
    "\n",
    "y_Train = Merged_trained_labels #Merged_trained_labels\n",
    "y_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85afeed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[158, 159, 165, ..., 124, 129, 110],\n",
       "       [235, 231, 232, ..., 178, 191, 199],\n",
       "       [158, 158, 139, ...,   8,   3,   7],\n",
       "       ...,\n",
       "       [ 20,  19,  15, ...,  50,  53,  47],\n",
       "       [ 25,  15,  23, ...,  80,  81,  80],\n",
       "       [ 73,  98,  99, ...,  94,  58,  26]], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Test = test_batch['data']\n",
    "X_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc3b013e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 0,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 0,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 9,\n",
       " 7,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 7,\n",
       " 2,\n",
       " 9,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 8,\n",
       " 7,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 9,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 7,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 0,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 2,\n",
       " 7,\n",
       " 6,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 7,\n",
       " 0,\n",
       " 8,\n",
       " 2,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 9,\n",
       " 0,\n",
       " 6,\n",
       " 1,\n",
       " 0,\n",
       " 9,\n",
       " 1,\n",
       " 0,\n",
       " 7,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 3,\n",
       " 8,\n",
       " 0,\n",
       " 7,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 3,\n",
       " 2,\n",
       " 8,\n",
       " 0,\n",
       " 9,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 9,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 2,\n",
       " 8,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 9,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 7,\n",
       " 9,\n",
       " 3,\n",
       " 6,\n",
       " 1,\n",
       " 9,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 9,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 7,\n",
       " 3,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 0,\n",
       " 5,\n",
       " 7,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 9,\n",
       " 4,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 9,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 7,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 2,\n",
       " 6,\n",
       " 7,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 9,\n",
       " 3,\n",
       " 4,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 2,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 7,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 9,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 8,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 9,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 9,\n",
       " 2,\n",
       " 9,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 5,\n",
       " 9,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 0,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 8,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 2,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 4,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 8,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 3,\n",
       " 0,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 7,\n",
       " 0,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 1,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 2,\n",
       " 9,\n",
       " 0,\n",
       " 4,\n",
       " 7,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 8,\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_Test = test_batch['labels']\n",
    "y_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e297bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3072)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fad01c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba05eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "import numpy as np\n",
    "tf.__version__\n",
    "\n",
    "X_Train = tf.constant(np.zeros((50000,3072)))\n",
    "\n",
    "X_Train.shape #TensorShape([50000, 3072])\n",
    "\n",
    "X_Train = tf.reshape(X_Train, [-1,32,32,3])\n",
    "\n",
    "X_Train.shape #TensorShape([50000, 32, 32, 3])\n",
    "\n",
    "\"\"\"\n",
    "#numpy.reshape(a, newshape)\n",
    "\n",
    "X_Train = X_Train.reshape(50000, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68186b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f98a9bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5b8ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# One Hot Encoding of Labels using to_categorical\n",
    "y_Train = to_categorical(y_Train, num_classes)\n",
    "y_Test = to_categorical(y_Test, num_classes)\n",
    "\n",
    "##Normalise the data by dividing the X files by 255 pixels\n",
    "X_Train = X_Train / 255.0\n",
    "X_Test = X_Test / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42ee66d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 15, 15, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 6, 6, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                65600     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122570 (478.79 KB)\n",
      "Trainable params: 122570 (478.79 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "1250/1250 [==============================] - 53s 41ms/step - loss: 2.0654 - accuracy: 0.2273 - val_loss: 2.0095 - val_accuracy: 0.2470\n",
      "Epoch 2/150\n",
      "1250/1250 [==============================] - 49s 39ms/step - loss: 1.8987 - accuracy: 0.2997 - val_loss: 1.8208 - val_accuracy: 0.3267\n",
      "Epoch 3/150\n",
      "1250/1250 [==============================] - 50s 40ms/step - loss: 1.8362 - accuracy: 0.3293 - val_loss: 1.8232 - val_accuracy: 0.3308\n",
      "Epoch 4/150\n",
      "1250/1250 [==============================] - 51s 41ms/step - loss: 1.7999 - accuracy: 0.3425 - val_loss: 1.7493 - val_accuracy: 0.3656\n",
      "Epoch 5/150\n",
      "1250/1250 [==============================] - 51s 40ms/step - loss: 1.7532 - accuracy: 0.3635 - val_loss: 1.7321 - val_accuracy: 0.3717\n",
      "Epoch 6/150\n",
      "1250/1250 [==============================] - 51s 41ms/step - loss: 1.7286 - accuracy: 0.3751 - val_loss: 1.7092 - val_accuracy: 0.3812\n",
      "Epoch 7/150\n",
      "1250/1250 [==============================] - 51s 41ms/step - loss: 1.7073 - accuracy: 0.3817 - val_loss: 1.6976 - val_accuracy: 0.3806\n",
      "Epoch 8/150\n",
      "1250/1250 [==============================] - 52s 41ms/step - loss: 1.6859 - accuracy: 0.3909 - val_loss: 1.6771 - val_accuracy: 0.3999\n",
      "Epoch 9/150\n",
      "1250/1250 [==============================] - 52s 41ms/step - loss: 1.6624 - accuracy: 0.4034 - val_loss: 1.6739 - val_accuracy: 0.4069\n",
      "Epoch 10/150\n",
      "1250/1250 [==============================] - 52s 41ms/step - loss: 1.6563 - accuracy: 0.4025 - val_loss: 1.6588 - val_accuracy: 0.4022\n",
      "Epoch 11/150\n",
      "1250/1250 [==============================] - 52s 41ms/step - loss: 1.6406 - accuracy: 0.4092 - val_loss: 1.6241 - val_accuracy: 0.4218\n",
      "Epoch 12/150\n",
      "1250/1250 [==============================] - 52s 41ms/step - loss: 1.6265 - accuracy: 0.4142 - val_loss: 1.6446 - val_accuracy: 0.4129\n",
      "Epoch 13/150\n",
      "1250/1250 [==============================] - 52s 41ms/step - loss: 1.6230 - accuracy: 0.4192 - val_loss: 1.5981 - val_accuracy: 0.4298\n",
      "Epoch 14/150\n",
      "1250/1250 [==============================] - 52s 41ms/step - loss: 1.6128 - accuracy: 0.4209 - val_loss: 1.5881 - val_accuracy: 0.4317\n",
      "Epoch 15/150\n",
      "1250/1250 [==============================] - 52s 41ms/step - loss: 1.6011 - accuracy: 0.4250 - val_loss: 1.6465 - val_accuracy: 0.4145\n",
      "Epoch 16/150\n",
      "1250/1250 [==============================] - 52s 42ms/step - loss: 1.5959 - accuracy: 0.4297 - val_loss: 1.5939 - val_accuracy: 0.4294\n",
      "Epoch 17/150\n",
      "1250/1250 [==============================] - 52s 42ms/step - loss: 1.5850 - accuracy: 0.4324 - val_loss: 1.5732 - val_accuracy: 0.4461\n",
      "Epoch 18/150\n",
      "1250/1250 [==============================] - 52s 42ms/step - loss: 1.5849 - accuracy: 0.4300 - val_loss: 1.5569 - val_accuracy: 0.4491\n",
      "Epoch 19/150\n",
      "1250/1250 [==============================] - 50s 40ms/step - loss: 1.5731 - accuracy: 0.4384 - val_loss: 1.6008 - val_accuracy: 0.4389\n",
      "Epoch 20/150\n",
      "1250/1250 [==============================] - 50s 40ms/step - loss: 1.5696 - accuracy: 0.4390 - val_loss: 1.6104 - val_accuracy: 0.4273\n",
      "Epoch 21/150\n",
      "1250/1250 [==============================] - 51s 41ms/step - loss: 1.5635 - accuracy: 0.4431 - val_loss: 1.5376 - val_accuracy: 0.4513\n",
      "Epoch 22/150\n",
      "1250/1250 [==============================] - 51s 41ms/step - loss: 1.5581 - accuracy: 0.4412 - val_loss: 1.5438 - val_accuracy: 0.4529\n",
      "Epoch 23/150\n",
      "1250/1250 [==============================] - 52s 41ms/step - loss: 1.5450 - accuracy: 0.4475 - val_loss: 1.5538 - val_accuracy: 0.4457\n",
      "Epoch 24/150\n",
      "1250/1250 [==============================] - 52s 41ms/step - loss: 1.5479 - accuracy: 0.4430 - val_loss: 1.5605 - val_accuracy: 0.4413\n",
      "Epoch 25/150\n",
      "1250/1250 [==============================] - 52s 41ms/step - loss: 1.5400 - accuracy: 0.4510 - val_loss: 1.5188 - val_accuracy: 0.4593\n",
      "Epoch 26/150\n",
      "1250/1250 [==============================] - 52s 41ms/step - loss: 1.5345 - accuracy: 0.4513 - val_loss: 1.5446 - val_accuracy: 0.4516\n",
      "Epoch 27/150\n",
      "1250/1250 [==============================] - 51s 41ms/step - loss: 1.5279 - accuracy: 0.4518 - val_loss: 1.5294 - val_accuracy: 0.4575\n",
      "Epoch 28/150\n",
      "1250/1250 [==============================] - 50s 40ms/step - loss: 1.5306 - accuracy: 0.4530 - val_loss: 1.5852 - val_accuracy: 0.4379\n",
      "Epoch 29/150\n",
      "1250/1250 [==============================] - 51s 41ms/step - loss: 1.5213 - accuracy: 0.4569 - val_loss: 1.5202 - val_accuracy: 0.4604\n",
      "Epoch 30/150\n",
      "1250/1250 [==============================] - 51s 40ms/step - loss: 1.5086 - accuracy: 0.4640 - val_loss: 1.5448 - val_accuracy: 0.4501\n",
      "Epoch 31/150\n",
      "1250/1250 [==============================] - 50s 40ms/step - loss: 1.5184 - accuracy: 0.4613 - val_loss: 1.5329 - val_accuracy: 0.4609\n",
      "Epoch 32/150\n",
      "1250/1250 [==============================] - 51s 40ms/step - loss: 1.5100 - accuracy: 0.4603 - val_loss: 1.5477 - val_accuracy: 0.4579\n",
      "Epoch 33/150\n",
      "1250/1250 [==============================] - 51s 41ms/step - loss: 1.4973 - accuracy: 0.4674 - val_loss: 1.5639 - val_accuracy: 0.4464\n",
      "Epoch 34/150\n",
      "1250/1250 [==============================] - 51s 41ms/step - loss: 1.5035 - accuracy: 0.4606 - val_loss: 1.4916 - val_accuracy: 0.4683\n",
      "Epoch 35/150\n",
      "1250/1250 [==============================] - 51s 41ms/step - loss: 1.4926 - accuracy: 0.4712 - val_loss: 1.5460 - val_accuracy: 0.4519\n",
      "Epoch 36/150\n",
      "1250/1250 [==============================] - 50s 40ms/step - loss: 1.4869 - accuracy: 0.4707 - val_loss: 1.5103 - val_accuracy: 0.4603\n",
      "Epoch 37/150\n",
      "1250/1250 [==============================] - 51s 41ms/step - loss: 1.4835 - accuracy: 0.4719 - val_loss: 1.5080 - val_accuracy: 0.4609\n",
      "Epoch 38/150\n",
      "1250/1250 [==============================] - 51s 40ms/step - loss: 1.4784 - accuracy: 0.4743 - val_loss: 1.4809 - val_accuracy: 0.4784\n",
      "Epoch 39/150\n",
      "1250/1250 [==============================] - 51s 41ms/step - loss: 1.4781 - accuracy: 0.4746 - val_loss: 1.4772 - val_accuracy: 0.4788\n",
      "Epoch 40/150\n",
      "1250/1250 [==============================] - 51s 41ms/step - loss: 1.4760 - accuracy: 0.4762 - val_loss: 1.4733 - val_accuracy: 0.4828\n",
      "Epoch 41/150\n",
      "1250/1250 [==============================] - 51s 41ms/step - loss: 1.4687 - accuracy: 0.4747 - val_loss: 1.5257 - val_accuracy: 0.4676\n",
      "Epoch 42/150\n",
      "1250/1250 [==============================] - 51s 41ms/step - loss: 1.4668 - accuracy: 0.4795 - val_loss: 1.5561 - val_accuracy: 0.4505\n",
      "Epoch 43/150\n",
      "1250/1250 [==============================] - 51s 41ms/step - loss: 1.4636 - accuracy: 0.4799 - val_loss: 1.4921 - val_accuracy: 0.4722\n",
      "Epoch 44/150\n",
      "1250/1250 [==============================] - 51s 40ms/step - loss: 1.4577 - accuracy: 0.4783 - val_loss: 1.4620 - val_accuracy: 0.4801\n",
      "Epoch 45/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.4558 - accuracy: 0.4816 - val_loss: 1.4496 - val_accuracy: 0.4871\n",
      "Epoch 46/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.4518 - accuracy: 0.4821 - val_loss: 1.4505 - val_accuracy: 0.4791\n",
      "Epoch 47/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4490 - accuracy: 0.4827 - val_loss: 1.4996 - val_accuracy: 0.4746\n",
      "Epoch 48/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4471 - accuracy: 0.4873 - val_loss: 1.4508 - val_accuracy: 0.4885\n",
      "Epoch 49/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4419 - accuracy: 0.4900 - val_loss: 1.4715 - val_accuracy: 0.4787\n",
      "Epoch 50/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4434 - accuracy: 0.4847 - val_loss: 1.4725 - val_accuracy: 0.4818\n",
      "Epoch 51/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4398 - accuracy: 0.4866 - val_loss: 1.4499 - val_accuracy: 0.4872\n",
      "Epoch 52/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.4432 - accuracy: 0.4866 - val_loss: 1.4804 - val_accuracy: 0.4743\n",
      "Epoch 53/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4363 - accuracy: 0.4882 - val_loss: 1.4568 - val_accuracy: 0.4841\n",
      "Epoch 54/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4352 - accuracy: 0.4909 - val_loss: 1.4793 - val_accuracy: 0.4762\n",
      "Epoch 55/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4331 - accuracy: 0.4930 - val_loss: 1.4298 - val_accuracy: 0.4945\n",
      "Epoch 56/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4297 - accuracy: 0.4917 - val_loss: 1.4362 - val_accuracy: 0.4931\n",
      "Epoch 57/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4263 - accuracy: 0.4951 - val_loss: 1.4514 - val_accuracy: 0.4855\n",
      "Epoch 58/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4212 - accuracy: 0.4967 - val_loss: 1.4572 - val_accuracy: 0.4868\n",
      "Epoch 59/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.4237 - accuracy: 0.4924 - val_loss: 1.4467 - val_accuracy: 0.4832\n",
      "Epoch 60/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.4182 - accuracy: 0.4963 - val_loss: 1.4539 - val_accuracy: 0.4849\n",
      "Epoch 61/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4203 - accuracy: 0.4965 - val_loss: 1.4459 - val_accuracy: 0.4880\n",
      "Epoch 62/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.4176 - accuracy: 0.4985 - val_loss: 1.4662 - val_accuracy: 0.4880\n",
      "Epoch 63/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4174 - accuracy: 0.4945 - val_loss: 1.4245 - val_accuracy: 0.4954\n",
      "Epoch 64/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4192 - accuracy: 0.4995 - val_loss: 1.4306 - val_accuracy: 0.4984\n",
      "Epoch 65/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.4135 - accuracy: 0.4994 - val_loss: 1.4466 - val_accuracy: 0.4910\n",
      "Epoch 66/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4132 - accuracy: 0.5001 - val_loss: 1.4395 - val_accuracy: 0.4832\n",
      "Epoch 67/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4138 - accuracy: 0.4972 - val_loss: 1.4436 - val_accuracy: 0.4908\n",
      "Epoch 68/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.4051 - accuracy: 0.5034 - val_loss: 1.4692 - val_accuracy: 0.4811\n",
      "Epoch 69/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.4144 - accuracy: 0.4952 - val_loss: 1.4364 - val_accuracy: 0.4899\n",
      "Epoch 70/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.4079 - accuracy: 0.4993 - val_loss: 1.4128 - val_accuracy: 0.4992\n",
      "Epoch 71/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.4021 - accuracy: 0.5022 - val_loss: 1.4435 - val_accuracy: 0.4860\n",
      "Epoch 72/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.4019 - accuracy: 0.5039 - val_loss: 1.4203 - val_accuracy: 0.5005\n",
      "Epoch 73/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3946 - accuracy: 0.5042 - val_loss: 1.4234 - val_accuracy: 0.4972\n",
      "Epoch 74/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.3920 - accuracy: 0.5076 - val_loss: 1.4663 - val_accuracy: 0.4805\n",
      "Epoch 75/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3890 - accuracy: 0.5051 - val_loss: 1.4229 - val_accuracy: 0.4946\n",
      "Epoch 76/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3912 - accuracy: 0.5064 - val_loss: 1.4232 - val_accuracy: 0.4993\n",
      "Epoch 77/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3908 - accuracy: 0.5065 - val_loss: 1.4157 - val_accuracy: 0.5017\n",
      "Epoch 78/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3950 - accuracy: 0.5062 - val_loss: 1.3974 - val_accuracy: 0.5064\n",
      "Epoch 79/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3915 - accuracy: 0.5070 - val_loss: 1.4384 - val_accuracy: 0.4941\n",
      "Epoch 80/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3866 - accuracy: 0.5077 - val_loss: 1.3805 - val_accuracy: 0.5127\n",
      "Epoch 81/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3897 - accuracy: 0.5055 - val_loss: 1.4178 - val_accuracy: 0.5051\n",
      "Epoch 82/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.3882 - accuracy: 0.5062 - val_loss: 1.4195 - val_accuracy: 0.4969\n",
      "Epoch 83/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3833 - accuracy: 0.5086 - val_loss: 1.4902 - val_accuracy: 0.4783\n",
      "Epoch 84/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3808 - accuracy: 0.5124 - val_loss: 1.4328 - val_accuracy: 0.4936\n",
      "Epoch 85/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3791 - accuracy: 0.5117 - val_loss: 1.4094 - val_accuracy: 0.4972\n",
      "Epoch 86/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3760 - accuracy: 0.5151 - val_loss: 1.4236 - val_accuracy: 0.4998\n",
      "Epoch 87/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.3832 - accuracy: 0.5099 - val_loss: 1.4034 - val_accuracy: 0.5093\n",
      "Epoch 88/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3801 - accuracy: 0.5110 - val_loss: 1.4214 - val_accuracy: 0.4965\n",
      "Epoch 89/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3754 - accuracy: 0.5110 - val_loss: 1.4623 - val_accuracy: 0.4947\n",
      "Epoch 90/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3757 - accuracy: 0.5105 - val_loss: 1.4294 - val_accuracy: 0.4922\n",
      "Epoch 91/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3699 - accuracy: 0.5134 - val_loss: 1.3899 - val_accuracy: 0.5116\n",
      "Epoch 92/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3764 - accuracy: 0.5117 - val_loss: 1.3891 - val_accuracy: 0.5039\n",
      "Epoch 93/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.3713 - accuracy: 0.5156 - val_loss: 1.4380 - val_accuracy: 0.4939\n",
      "Epoch 94/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.3769 - accuracy: 0.5106 - val_loss: 1.4087 - val_accuracy: 0.5000\n",
      "Epoch 95/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3753 - accuracy: 0.5119 - val_loss: 1.4039 - val_accuracy: 0.5033\n",
      "Epoch 96/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.3664 - accuracy: 0.5154 - val_loss: 1.4054 - val_accuracy: 0.5033\n",
      "Epoch 97/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3672 - accuracy: 0.5164 - val_loss: 1.4092 - val_accuracy: 0.5042\n",
      "Epoch 98/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3713 - accuracy: 0.5154 - val_loss: 1.4100 - val_accuracy: 0.5022\n",
      "Epoch 99/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.3622 - accuracy: 0.5163 - val_loss: 1.3811 - val_accuracy: 0.5160\n",
      "Epoch 100/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3640 - accuracy: 0.5154 - val_loss: 1.4130 - val_accuracy: 0.4958\n",
      "Epoch 101/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3665 - accuracy: 0.5140 - val_loss: 1.4539 - val_accuracy: 0.4876\n",
      "Epoch 102/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3605 - accuracy: 0.5206 - val_loss: 1.4059 - val_accuracy: 0.5041\n",
      "Epoch 103/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3642 - accuracy: 0.5177 - val_loss: 1.4040 - val_accuracy: 0.5047\n",
      "Epoch 104/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3632 - accuracy: 0.5163 - val_loss: 1.4183 - val_accuracy: 0.4955\n",
      "Epoch 105/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3618 - accuracy: 0.5179 - val_loss: 1.3910 - val_accuracy: 0.5030\n",
      "Epoch 106/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3575 - accuracy: 0.5208 - val_loss: 1.3773 - val_accuracy: 0.5137\n",
      "Epoch 107/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3577 - accuracy: 0.5211 - val_loss: 1.3971 - val_accuracy: 0.5060\n",
      "Epoch 108/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.3588 - accuracy: 0.5200 - val_loss: 1.4109 - val_accuracy: 0.5024\n",
      "Epoch 109/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.3557 - accuracy: 0.5208 - val_loss: 1.3992 - val_accuracy: 0.5100\n",
      "Epoch 110/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3521 - accuracy: 0.5200 - val_loss: 1.3862 - val_accuracy: 0.5170\n",
      "Epoch 111/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3529 - accuracy: 0.5204 - val_loss: 1.4123 - val_accuracy: 0.5024\n",
      "Epoch 112/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3638 - accuracy: 0.5184 - val_loss: 1.3941 - val_accuracy: 0.5153\n",
      "Epoch 113/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3569 - accuracy: 0.5225 - val_loss: 1.3712 - val_accuracy: 0.5196\n",
      "Epoch 114/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3498 - accuracy: 0.5220 - val_loss: 1.3805 - val_accuracy: 0.5181\n",
      "Epoch 115/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3535 - accuracy: 0.5201 - val_loss: 1.3818 - val_accuracy: 0.5131\n",
      "Epoch 116/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3539 - accuracy: 0.5214 - val_loss: 1.3634 - val_accuracy: 0.5168\n",
      "Epoch 117/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3467 - accuracy: 0.5227 - val_loss: 1.3876 - val_accuracy: 0.5152\n",
      "Epoch 118/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3469 - accuracy: 0.5249 - val_loss: 1.3733 - val_accuracy: 0.5152\n",
      "Epoch 119/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3471 - accuracy: 0.5240 - val_loss: 1.4178 - val_accuracy: 0.5035\n",
      "Epoch 120/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3510 - accuracy: 0.5237 - val_loss: 1.4304 - val_accuracy: 0.4985\n",
      "Epoch 121/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3447 - accuracy: 0.5217 - val_loss: 1.3883 - val_accuracy: 0.5109\n",
      "Epoch 122/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3513 - accuracy: 0.5189 - val_loss: 1.3754 - val_accuracy: 0.5157\n",
      "Epoch 123/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3399 - accuracy: 0.5278 - val_loss: 1.3555 - val_accuracy: 0.5243\n",
      "Epoch 124/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.3446 - accuracy: 0.5252 - val_loss: 1.4203 - val_accuracy: 0.5038\n",
      "Epoch 125/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3510 - accuracy: 0.5227 - val_loss: 1.3538 - val_accuracy: 0.5261\n",
      "Epoch 126/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3476 - accuracy: 0.5224 - val_loss: 1.3828 - val_accuracy: 0.5170\n",
      "Epoch 127/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.3384 - accuracy: 0.5275 - val_loss: 1.5183 - val_accuracy: 0.4698\n",
      "Epoch 128/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3415 - accuracy: 0.5275 - val_loss: 1.3605 - val_accuracy: 0.5156\n",
      "Epoch 129/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3467 - accuracy: 0.5212 - val_loss: 1.3625 - val_accuracy: 0.5204\n",
      "Epoch 130/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3448 - accuracy: 0.5251 - val_loss: 1.3850 - val_accuracy: 0.5094\n",
      "Epoch 131/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3329 - accuracy: 0.5286 - val_loss: 1.3747 - val_accuracy: 0.5142\n",
      "Epoch 132/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3389 - accuracy: 0.5242 - val_loss: 1.3743 - val_accuracy: 0.5163\n",
      "Epoch 133/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3431 - accuracy: 0.5258 - val_loss: 1.3996 - val_accuracy: 0.5077\n",
      "Epoch 134/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3402 - accuracy: 0.5264 - val_loss: 1.4035 - val_accuracy: 0.5160\n",
      "Epoch 135/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3473 - accuracy: 0.5247 - val_loss: 1.3981 - val_accuracy: 0.5068\n",
      "Epoch 136/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.3353 - accuracy: 0.5273 - val_loss: 1.3491 - val_accuracy: 0.5245\n",
      "Epoch 137/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3414 - accuracy: 0.5242 - val_loss: 1.3568 - val_accuracy: 0.5228\n",
      "Epoch 138/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3396 - accuracy: 0.5249 - val_loss: 1.3975 - val_accuracy: 0.5109\n",
      "Epoch 139/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.3414 - accuracy: 0.5274 - val_loss: 1.3854 - val_accuracy: 0.5123\n",
      "Epoch 140/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3342 - accuracy: 0.5271 - val_loss: 1.4060 - val_accuracy: 0.5035\n",
      "Epoch 141/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.3282 - accuracy: 0.5294 - val_loss: 1.3931 - val_accuracy: 0.5039\n",
      "Epoch 142/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.3393 - accuracy: 0.5263 - val_loss: 1.3556 - val_accuracy: 0.5179\n",
      "Epoch 143/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3306 - accuracy: 0.5281 - val_loss: 1.3965 - val_accuracy: 0.5065\n",
      "Epoch 144/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3270 - accuracy: 0.5311 - val_loss: 1.3846 - val_accuracy: 0.5133\n",
      "Epoch 145/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3308 - accuracy: 0.5289 - val_loss: 1.3875 - val_accuracy: 0.5090\n",
      "Epoch 146/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.3310 - accuracy: 0.5306 - val_loss: 1.3754 - val_accuracy: 0.5211\n",
      "Epoch 147/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.3339 - accuracy: 0.5279 - val_loss: 1.3631 - val_accuracy: 0.5188\n",
      "Epoch 148/150\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 1.3257 - accuracy: 0.5286 - val_loss: 1.3601 - val_accuracy: 0.5137\n",
      "Epoch 149/150\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 1.3371 - accuracy: 0.5282 - val_loss: 1.3498 - val_accuracy: 0.5225\n",
      "Epoch 150/150\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 1.3295 - accuracy: 0.5309 - val_loss: 1.3725 - val_accuracy: 0.5204\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the Image Data Generator Class with the Data Augmentation\n",
    "datagen = ImageDataGenerator(width_shift_range = 0.2, height_shift_range = 0.2, \n",
    "                             rotation_range = 20, horizontal_flip = True, \n",
    "                             vertical_flip = True, validation_split = 0.2)\n",
    "\n",
    "# Apply the Data Augmentation to the Training Images\n",
    "#datagen.fit(X_Train)\n",
    "datagen.fit(X_Train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the Generator for the Training Images\n",
    "train_gen = datagen.flow(X_Train, y_Train, batch_size = 32, \n",
    "                         subset = 'training')\n",
    "\n",
    "# Create the Generator for the Validation Images\n",
    "val_gen = datagen.flow(X_Train, y_Train, batch_size = 8, \n",
    "                         subset = 'validation')\n",
    "\n",
    "\n",
    "img_height = 32\n",
    "img_width = 32\n",
    "\n",
    "# Building the Keras Model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dropout(rate = 0.2))\n",
    "model.add(Dense(units = num_classes, activation = 'softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "steps_per_epoch = len(X_Train) * 0.8//32\n",
    "\n",
    "history = model.fit(train_gen, validation_data = val_gen, \n",
    "          steps_per_epoch = steps_per_epoch, epochs = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "cd116397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 15, 15, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 6, 6, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                65600     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122570 (478.79 KB)\n",
      "Trainable params: 122570 (478.79 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 271s 216ms/step - loss: 2.0642 - accuracy: 0.2208 - val_loss: 2.0151 - val_accuracy: 0.2398\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 305s 244ms/step - loss: 1.9139 - accuracy: 0.2941 - val_loss: 1.8841 - val_accuracy: 0.3087\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 289s 231ms/step - loss: 1.8560 - accuracy: 0.3169 - val_loss: 1.8366 - val_accuracy: 0.3297\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 287s 229ms/step - loss: 1.8127 - accuracy: 0.3361 - val_loss: 1.7756 - val_accuracy: 0.3577\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 279s 223ms/step - loss: 1.7871 - accuracy: 0.3494 - val_loss: 1.7531 - val_accuracy: 0.3662\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 285s 228ms/step - loss: 1.7556 - accuracy: 0.3629 - val_loss: 1.7175 - val_accuracy: 0.3768\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 280s 224ms/step - loss: 1.7360 - accuracy: 0.3732 - val_loss: 1.7094 - val_accuracy: 0.3795\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 279s 223ms/step - loss: 1.7203 - accuracy: 0.3767 - val_loss: 1.7626 - val_accuracy: 0.3651\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 283s 226ms/step - loss: 1.7077 - accuracy: 0.3826 - val_loss: 1.7142 - val_accuracy: 0.3804\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 279s 223ms/step - loss: 1.6905 - accuracy: 0.3866 - val_loss: 1.6919 - val_accuracy: 0.3867\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the Image Data Generator Class with the Data Augmentation\n",
    "datagen = ImageDataGenerator(width_shift_range = 0.2, height_shift_range = 0.2, \n",
    "                             rotation_range = 20, horizontal_flip = True, \n",
    "                             vertical_flip = True, validation_split = 0.2)\n",
    "\n",
    "# Apply the Data Augmentation to the Training Images\n",
    "#datagen.fit(X_Train)\n",
    "datagen.fit(X_Train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the Generator for the Training Images\n",
    "train_gen = datagen.flow(X_Train, y_Train, batch_size = 32, \n",
    "                         subset = 'training')\n",
    "\n",
    "# Create the Generator for the Validation Images\n",
    "val_gen = datagen.flow(X_Train, y_Train, batch_size = 8, \n",
    "                         subset = 'validation')\n",
    "\n",
    "\n",
    "img_height = 32\n",
    "img_width = 32\n",
    "\n",
    "# Building the Keras Model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dropout(rate = 0.2))\n",
    "model.add(Dense(units = num_classes, activation = 'softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "steps_per_epoch = len(X_Train) * 0.8//32\n",
    "\n",
    "history = model.fit(train_gen, validation_data = val_gen, \n",
    "          steps_per_epoch = steps_per_epoch, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f0080374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_10 (Conv2D)          (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 15, 15, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 6, 6, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                65600     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122570 (478.79 KB)\n",
      "Trainable params: 122570 (478.79 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1250/1250 [==============================] - 143s 114ms/step - loss: 2.0701 - accuracy: 0.2159 - val_loss: 1.8965 - val_accuracy: 0.2926\n",
      "Epoch 2/50\n",
      "1250/1250 [==============================] - 144s 115ms/step - loss: 1.9081 - accuracy: 0.2939 - val_loss: 1.8493 - val_accuracy: 0.3196\n",
      "Epoch 3/50\n",
      "1250/1250 [==============================] - 146s 117ms/step - loss: 1.8486 - accuracy: 0.3237 - val_loss: 1.7968 - val_accuracy: 0.3476\n",
      "Epoch 4/50\n",
      "1250/1250 [==============================] - 143s 115ms/step - loss: 1.8067 - accuracy: 0.3431 - val_loss: 1.7976 - val_accuracy: 0.3478\n",
      "Epoch 5/50\n",
      "1250/1250 [==============================] - 145s 116ms/step - loss: 1.7791 - accuracy: 0.3534 - val_loss: 1.7681 - val_accuracy: 0.3531\n",
      "Epoch 6/50\n",
      "1250/1250 [==============================] - 148s 118ms/step - loss: 1.7480 - accuracy: 0.3637 - val_loss: 1.7181 - val_accuracy: 0.3769\n",
      "Epoch 7/50\n",
      "1250/1250 [==============================] - 147s 118ms/step - loss: 1.7251 - accuracy: 0.3751 - val_loss: 1.6958 - val_accuracy: 0.3854\n",
      "Epoch 8/50\n",
      "1250/1250 [==============================] - 146s 117ms/step - loss: 1.7055 - accuracy: 0.3826 - val_loss: 1.7057 - val_accuracy: 0.3813\n",
      "Epoch 9/50\n",
      "1250/1250 [==============================] - 147s 117ms/step - loss: 1.6978 - accuracy: 0.3849 - val_loss: 1.7372 - val_accuracy: 0.3707\n",
      "Epoch 10/50\n",
      "1250/1250 [==============================] - 147s 117ms/step - loss: 1.6864 - accuracy: 0.3879 - val_loss: 1.6446 - val_accuracy: 0.4108\n",
      "Epoch 11/50\n",
      "1250/1250 [==============================] - 148s 119ms/step - loss: 1.6777 - accuracy: 0.3939 - val_loss: 1.6568 - val_accuracy: 0.4083\n",
      "Epoch 12/50\n",
      "1250/1250 [==============================] - 147s 117ms/step - loss: 1.6648 - accuracy: 0.3997 - val_loss: 1.6742 - val_accuracy: 0.4022\n",
      "Epoch 13/50\n",
      "1250/1250 [==============================] - 147s 118ms/step - loss: 1.6502 - accuracy: 0.4034 - val_loss: 1.6725 - val_accuracy: 0.4025\n",
      "Epoch 14/50\n",
      "1250/1250 [==============================] - 147s 117ms/step - loss: 1.6498 - accuracy: 0.4033 - val_loss: 1.6185 - val_accuracy: 0.4221\n",
      "Epoch 15/50\n",
      "1250/1250 [==============================] - 149s 119ms/step - loss: 1.6341 - accuracy: 0.4086 - val_loss: 1.6413 - val_accuracy: 0.4116\n",
      "Epoch 16/50\n",
      "1250/1250 [==============================] - 146s 117ms/step - loss: 1.6291 - accuracy: 0.4116 - val_loss: 1.6264 - val_accuracy: 0.4109\n",
      "Epoch 17/50\n",
      "1250/1250 [==============================] - 148s 118ms/step - loss: 1.6325 - accuracy: 0.4124 - val_loss: 1.6083 - val_accuracy: 0.4263\n",
      "Epoch 18/50\n",
      "1250/1250 [==============================] - 146s 117ms/step - loss: 1.6242 - accuracy: 0.4178 - val_loss: 1.5895 - val_accuracy: 0.4261\n",
      "Epoch 19/50\n",
      "1250/1250 [==============================] - 149s 119ms/step - loss: 1.6161 - accuracy: 0.4172 - val_loss: 1.6096 - val_accuracy: 0.4303\n",
      "Epoch 20/50\n",
      "1250/1250 [==============================] - 147s 117ms/step - loss: 1.6029 - accuracy: 0.4250 - val_loss: 1.5940 - val_accuracy: 0.4285\n",
      "Epoch 21/50\n",
      "1250/1250 [==============================] - 148s 118ms/step - loss: 1.6017 - accuracy: 0.4251 - val_loss: 1.5882 - val_accuracy: 0.4318\n",
      "Epoch 22/50\n",
      "1250/1250 [==============================] - 147s 117ms/step - loss: 1.6014 - accuracy: 0.4271 - val_loss: 1.5831 - val_accuracy: 0.4300\n",
      "Epoch 23/50\n",
      "1250/1250 [==============================] - 147s 118ms/step - loss: 1.5951 - accuracy: 0.4291 - val_loss: 1.6193 - val_accuracy: 0.4170\n",
      "Epoch 24/50\n",
      "1250/1250 [==============================] - 147s 117ms/step - loss: 1.5917 - accuracy: 0.4286 - val_loss: 1.5995 - val_accuracy: 0.4266\n",
      "Epoch 25/50\n",
      "1250/1250 [==============================] - 147s 118ms/step - loss: 1.5800 - accuracy: 0.4329 - val_loss: 1.5609 - val_accuracy: 0.4471\n",
      "Epoch 26/50\n",
      "1250/1250 [==============================] - 148s 118ms/step - loss: 1.5806 - accuracy: 0.4324 - val_loss: 1.5879 - val_accuracy: 0.4347\n",
      "Epoch 27/50\n",
      "1250/1250 [==============================] - 146s 117ms/step - loss: 1.5765 - accuracy: 0.4335 - val_loss: 1.6001 - val_accuracy: 0.4288\n",
      "Epoch 28/50\n",
      "1250/1250 [==============================] - 144s 115ms/step - loss: 1.5724 - accuracy: 0.4374 - val_loss: 1.5754 - val_accuracy: 0.4424\n",
      "Epoch 29/50\n",
      "1250/1250 [==============================] - 145s 116ms/step - loss: 1.5699 - accuracy: 0.4334 - val_loss: 1.5739 - val_accuracy: 0.4355\n",
      "Epoch 30/50\n",
      "1250/1250 [==============================] - 146s 117ms/step - loss: 1.5631 - accuracy: 0.4423 - val_loss: 1.5622 - val_accuracy: 0.4423\n",
      "Epoch 31/50\n",
      "1250/1250 [==============================] - 146s 117ms/step - loss: 1.5617 - accuracy: 0.4409 - val_loss: 1.5429 - val_accuracy: 0.4449\n",
      "Epoch 32/50\n",
      "1250/1250 [==============================] - 144s 115ms/step - loss: 1.5560 - accuracy: 0.4441 - val_loss: 1.5695 - val_accuracy: 0.4462\n",
      "Epoch 33/50\n",
      "1250/1250 [==============================] - 145s 116ms/step - loss: 1.5503 - accuracy: 0.4405 - val_loss: 1.5868 - val_accuracy: 0.4346\n",
      "Epoch 34/50\n",
      "1250/1250 [==============================] - 144s 115ms/step - loss: 1.5488 - accuracy: 0.4460 - val_loss: 1.5674 - val_accuracy: 0.4460\n",
      "Epoch 35/50\n",
      "1250/1250 [==============================] - 146s 117ms/step - loss: 1.5445 - accuracy: 0.4460 - val_loss: 1.5480 - val_accuracy: 0.4488\n",
      "Epoch 36/50\n",
      "1250/1250 [==============================] - 145s 116ms/step - loss: 1.5491 - accuracy: 0.4440 - val_loss: 1.5837 - val_accuracy: 0.4365\n",
      "Epoch 37/50\n",
      "1250/1250 [==============================] - 147s 117ms/step - loss: 1.5390 - accuracy: 0.4500 - val_loss: 1.5571 - val_accuracy: 0.4505\n",
      "Epoch 38/50\n",
      "1250/1250 [==============================] - 145s 116ms/step - loss: 1.5355 - accuracy: 0.4508 - val_loss: 1.6290 - val_accuracy: 0.4232\n",
      "Epoch 39/50\n",
      "1250/1250 [==============================] - 146s 117ms/step - loss: 1.5327 - accuracy: 0.4533 - val_loss: 1.5512 - val_accuracy: 0.4462\n",
      "Epoch 40/50\n",
      "1250/1250 [==============================] - 145s 116ms/step - loss: 1.5303 - accuracy: 0.4545 - val_loss: 1.5409 - val_accuracy: 0.4550\n",
      "Epoch 41/50\n",
      "1250/1250 [==============================] - 144s 115ms/step - loss: 1.5308 - accuracy: 0.4519 - val_loss: 1.5245 - val_accuracy: 0.4520\n",
      "Epoch 42/50\n",
      "1250/1250 [==============================] - 145s 116ms/step - loss: 1.5328 - accuracy: 0.4518 - val_loss: 1.5244 - val_accuracy: 0.4548\n",
      "Epoch 43/50\n",
      "1250/1250 [==============================] - 146s 117ms/step - loss: 1.5285 - accuracy: 0.4534 - val_loss: 1.5205 - val_accuracy: 0.4667\n",
      "Epoch 44/50\n",
      "1250/1250 [==============================] - 145s 116ms/step - loss: 1.5256 - accuracy: 0.4542 - val_loss: 1.5162 - val_accuracy: 0.4654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50\n",
      "1250/1250 [==============================] - 139s 111ms/step - loss: 1.5230 - accuracy: 0.4580 - val_loss: 1.5220 - val_accuracy: 0.4633\n",
      "Epoch 46/50\n",
      "1250/1250 [==============================] - 138s 110ms/step - loss: 1.5234 - accuracy: 0.4511 - val_loss: 1.5334 - val_accuracy: 0.4560\n",
      "Epoch 47/50\n",
      "1250/1250 [==============================] - 138s 111ms/step - loss: 1.5201 - accuracy: 0.4563 - val_loss: 1.5316 - val_accuracy: 0.4561\n",
      "Epoch 48/50\n",
      "1250/1250 [==============================] - 138s 110ms/step - loss: 1.5161 - accuracy: 0.4611 - val_loss: 1.4917 - val_accuracy: 0.4727\n",
      "Epoch 49/50\n",
      "1250/1250 [==============================] - 139s 111ms/step - loss: 1.5168 - accuracy: 0.4565 - val_loss: 1.4881 - val_accuracy: 0.4702\n",
      "Epoch 50/50\n",
      "1250/1250 [==============================] - 138s 110ms/step - loss: 1.5151 - accuracy: 0.4596 - val_loss: 1.5575 - val_accuracy: 0.4456\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the Image Data Generator Class with the Data Augmentation\n",
    "datagen = ImageDataGenerator(width_shift_range = 0.2, height_shift_range = 0.2, \n",
    "                             rotation_range = 20, horizontal_flip = True, \n",
    "                             vertical_flip = True, validation_split = 0.2)\n",
    "\n",
    "# Apply the Data Augmentation to the Training Images\n",
    "#datagen.fit(X_Train)\n",
    "datagen.fit(X_Train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the Generator for the Training Images\n",
    "train_gen = datagen.flow(X_Train, y_Train, batch_size = 32, \n",
    "                         subset = 'training')\n",
    "\n",
    "# Create the Generator for the Validation Images\n",
    "val_gen = datagen.flow(X_Train, y_Train, batch_size = 8, \n",
    "                         subset = 'validation')\n",
    "\n",
    "\n",
    "img_height = 32\n",
    "img_width = 32\n",
    "\n",
    "# Building the Keras Model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dropout(rate = 0.2))\n",
    "model.add(Dense(units = num_classes, activation = 'softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "steps_per_epoch = len(X_Train) * 0.8//32\n",
    "\n",
    "history = model.fit(train_gen, validation_data = val_gen, \n",
    "          steps_per_epoch = steps_per_epoch, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2ba78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21063beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "964182ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"sequential_3\" is incompatible with the layer: expected shape=(None, 32, 32, 3), found shape=(64, 3072)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[161], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m \tsummarize_diagnostics(history)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# entry point, run the test harness\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m run_test_harness()\n",
      "Cell \u001b[1;32mIn[161], line 49\u001b[0m, in \u001b[0;36mrun_test_harness\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m model \u001b[38;5;241m=\u001b[39m define_model()\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# fit model\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_Train, y_Train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_Test, y_Test), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# evaluate model\u001b[39;00m\n\u001b[0;32m     51\u001b[0m _, acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_Test, y_Test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\input_spec.py:298\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 298\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    299\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplay_shape(x\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"sequential_3\" is incompatible with the layer: expected shape=(None, 32, 32, 3), found shape=(64, 3072)"
     ]
    }
   ],
   "source": [
    "# test harness for evaluating models on the cifar10 dataset\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Flatten(input_shape=(none,32,32,3))) # 32*32*3 = 3072\n",
    "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.001, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "\t# plot loss\n",
    "\tpyplot.subplot(211)\n",
    "\tpyplot.title('Cross Entropy Loss')\n",
    "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\t# plot accuracy\n",
    "\tpyplot.subplot(212)\n",
    "\tpyplot.title('Classification Accuracy')\n",
    "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "\t# save plot to file\n",
    "\tfilename = sys.argv[0].split('/')[-1]\n",
    "\tpyplot.savefig(filename + '_plot.png')\n",
    "\tpyplot.close()\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\tmodel = define_model()\n",
    "\t# fit model\n",
    "\thistory = model.fit(X_Train, y_Train, epochs=10, batch_size=64, validation_data=(X_Test, y_Test), verbose=0)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(X_Test, y_Test, verbose=0)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# learning curves\n",
    "\tsummarize_diagnostics(history)\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878e42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading the Data from the in built library\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "# Normalize the Pixel Data\n",
    "train_images = train_images/255.0\n",
    "test_images = test_images/255.0\n",
    "\n",
    "# Instantiate the Image Data Generator Class with the Data Augmentation\n",
    "datagen = ImageDataGenerator(width_shift_range = 0.2, height_shift_range = 0.2, \n",
    "                             rotation_range = 20, horizontal_flip = True, \n",
    "                             vertical_flip = True, validation_split = 0.2)\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# One Hot Encoding of Labels using to_categorical\n",
    "train_labels = to_categorical(train_labels, num_classes)\n",
    "test_labels = to_categorical(test_labels, num_classes)\n",
    "\n",
    "\n",
    "# Apply the Data Augmentation to the Training Images\n",
    "datagen.fit(train_images)\n",
    "\n",
    "# Create the Generator for the Training Images\n",
    "train_gen = datagen.flow(train_images, train_labels, batch_size = 32, \n",
    "                         subset = 'training')\n",
    "\n",
    "# Create the Generator for the Validation Images\n",
    "val_gen = datagen.flow(train_images, train_labels, batch_size = 8, \n",
    "                         subset = 'validation')\n",
    "\n",
    "\n",
    "img_height = 32\n",
    "img_width = 32\n",
    "\n",
    "# Building the Keras Model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dropout(rate = 0.2))\n",
    "model.add(Dense(units = num_classes, activation = 'softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "steps_per_epoch = len(train_images) * 0.8//32\n",
    "\n",
    "history = model.fit(train_gen, validation_data = val_gen, \n",
    "          steps_per_epoch = steps_per_epoch, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843cb1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_Train = to_categorical(y_Train,10)\n",
    "y_Test = to_categorical(y_Test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa9392",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f2325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32,(5,5),activation = 'relu', input_shape = (32,32,3)))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Conv2D(32,(5,5),activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1500, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(750, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(300, activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45179930",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', \n",
    "metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0e3b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hist = model.fit(X_Train,y_Train, batch_size = 64 , epochs = 2, \n",
    "validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a769ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hist = model.fit(X_Train,y_Train, batch_size = 64 , epochs = 100, \n",
    "validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee3aea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Load the pre-trained VGG16 model (excluding the top layer)\n",
    "vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e35af50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "# Create a sequential model\n",
    "custom_model = Sequential()\n",
    "\n",
    "# Add the VGG16 model (excluding the top layer)\n",
    "custom_model.add(vgg16_model)\n",
    "\n",
    "# Flatten the output from VGG16\n",
    "custom_model.add(Flatten())\n",
    "\n",
    "# Add a fully connected layer\n",
    "custom_model.add(Dense(256, activation='relu'))\n",
    "custom_model.add(Dropout(0.5))\n",
    "\n",
    "# Add the output layer for CIFAR-10 classification (10 classes)\n",
    "custom_model.add(Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053c08e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "custom_model.compile(loss='categorical_crossentropy',\n",
    "                     optimizer='adam',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "custom_model.fit(X_Train, y_Train, batch_size=64, epochs=10, validation_data=(X_Test, y_Test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cbff78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a9f9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ddc853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595dc50c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d949b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd273e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67464e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b4a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a289372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6e0bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e926ad90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895ea6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = combined_dict['data']\n",
    "print(\"Shape before reshape:\", X_train.shape)\n",
    "# Reshape the whole image data\n",
    "X_train = X_train.reshape(len(X_train),3,32,32)\n",
    "print(\"Shape after reshape and before transpose:\", X_train.shape)\n",
    "# Transpose the whole data\n",
    "X_train = X_train.transpose(0,2,3,1)\n",
    "print(\"Shape after reshape and transpose:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eae1fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a751664",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c5e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cc2368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be391a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ada9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# example of loading the cifar10 dataset\n",
    "from keras.utils import to_categorical\n",
    "Y_train = to_categorical(Y_train)\n",
    "y_test = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeccba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale pixels\n",
    "def prep_pixels(train, test):\n",
    "\t# convert from integers to floats\n",
    "\ttrain_norm = train.astype('float32')\n",
    "\ttest_norm = test.astype('float32')\n",
    "\t# normalize to range 0-1\n",
    "\ttrain_norm = train_norm / 255.0\n",
    "\ttest_norm = test_norm / 255.0\n",
    "\t# return normalized images\n",
    "\treturn train_norm, test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85df71d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cnn model\n",
    "def define_model():\n",
    "\tmodel = Sequential()\n",
    "\t# ...\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef16610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "_, acc = model.evaluate(testX, testY, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362061f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "\t# plot loss\n",
    "\tpyplot.subplot(211)\n",
    "\tpyplot.title('Cross Entropy Loss')\n",
    "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\t# plot accuracy\n",
    "\tpyplot.subplot(212)\n",
    "\tpyplot.title('Classification Accuracy')\n",
    "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "\t# save plot to file\n",
    "\tfilename = sys.argv[0].split('/')[-1]\n",
    "\tpyplot.savefig(filename + '_plot.png')\n",
    "\tpyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('> %.3f' % (acc * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287b9ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# load dataset\n",
    "\tX_train, Y_train, X_test, y_test = load_dataset()\n",
    "\t# prepare pixel data\n",
    "\tX_train, X_test = prep_pixels(X_train, X_test)\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# fit model\n",
    "\thistory = model.fit(X_train, Y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test), verbose=0)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(testX, X_test, verbose=0)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# learning curves\n",
    "\tsummarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcb51a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test harness for evaluating models on the cifar10 dataset\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "\t# load dataset\n",
    "\t(X_train, Y_train), (X_test, y_test) = load_dataset()\n",
    "\t# one hot encode target values\n",
    "\tY_train = to_categorical(Y_train)\n",
    "\ty_test = to_categorical(y_test)\n",
    "\treturn X_train, Y_train, X_test, y_test\n",
    "\n",
    "# scale pixels\n",
    "def prep_pixels(train, test):\n",
    "\t# convert from integers to floats\n",
    "\ttrain_norm = train.astype('float32')\n",
    "\ttest_norm = test.astype('float32')\n",
    "\t# normalize to range 0-1\n",
    "\ttrain_norm = train_norm / 255.0\n",
    "\ttest_norm = test_norm / 255.0\n",
    "\t# return normalized images\n",
    "\treturn train_norm, test_norm\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "\tmodel = Sequential()\n",
    "\t# ...\n",
    "\treturn model\n",
    "\n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "\t# plot loss\n",
    "\tpyplot.subplot(211)\n",
    "\tpyplot.title('Cross Entropy Loss')\n",
    "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\t# plot accuracy\n",
    "\tpyplot.subplot(212)\n",
    "\tpyplot.title('Classification Accuracy')\n",
    "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "\t# save plot to file\n",
    "\tfilename = sys.argv[0].split('/')[-1]\n",
    "\tpyplot.savefig(filename + '_plot.png')\n",
    "\tpyplot.close()\n",
    "\n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "\t# load dataset\n",
    "\tX_train, Y_train, X_test, y_test = load_dataset()\n",
    "\t# prepare pixel data\n",
    "\tX_train, X_test = prep_pixels(X_train, X_test)\n",
    "\t# define model\n",
    "\tmodel = define_model()\n",
    "\t# fit model\n",
    "\thistory = model.fit(X_train, Y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test), verbose=0)\n",
    "\t# evaluate model\n",
    "\t_, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\tprint('> %.3f' % (acc * 100.0))\n",
    "\t# learning curves\n",
    "\tsummarize_diagnostics(history)\n",
    "\n",
    "# entry point, run the test harness\n",
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7bb284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e5d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d11319",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(32, activation='relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6784d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(Y_train.shape[1], activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e8675",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0044b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb4ff4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
